{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input files\n",
    "#adir_aviso = 'F:/data/project_data/NASA_biophysical/aviso/'\n",
    "#filename_northpac_eddies=adir_aviso + 'eddy_trajectory_19930101_20170106_north_pacific_2020_10_06.nc'\n",
    "#filename_cpr_eddy=adir_data + 'eddy_cpr_data_north_pacific_2020_10_06.nc'\n",
    "#filename_aviso_all=adir_aviso+'/eddy_trajectory_19930101_20170106.nc'   #From AVISO  website\n",
    "#filename_cpr_all=adir_data + '/All CPR Sample catalogue with eddy info_2020_10_06.nc'\n",
    "#filename_cpr_all=adir_data+'All_CPR_Sample_catalogue_with_eddy_info_2020_10_06_origin_file.nc'\n",
    "\n",
    "ctype = 'goa'  #'CPR'\n",
    "if ctype=='goa':\n",
    "    adir_data = 'f:/data/project_data/NASA_biophysical/collocated_data/'\n",
    "    filename_origin_in=adir_data + 'NPPSD_GOA_allseabird_full_eddy_info.nc'\n",
    "    filename_origin_out=adir_data + 'NPPSD_GOA_allseabird_full_eddy_info_envdata'\n",
    "else:\n",
    "    adir_data = 'F:/data/project_data/NASA_biophysical/collocated_data/CPR/'\n",
    "    filename_origin_in=adir_data+'All_CPR_Sample_catalogue_with_eddy_info_2020_10_06_origin_file.nc'\n",
    "    filename_origin_out=adir_data+'All_CPR_Sample_catalogue_with_eddy_info_2020_10_06_origin_file_with_data'\n",
    "\n",
    "#################################################################################\n",
    "#some of the data is on pangeo gcp, some on AWS\n",
    "import sys\n",
    "sys.path.append('./../cloud_science/subroutines/')  #where your\n",
    "#from get_data_pangeo import get_data\n",
    "from get_data_local import get_data_360\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data and put in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,clim = get_data_360()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bird = xr.open_dataset(filename_origin_in)\n",
    "ds_bird['time'] = ds_bird.time64 if ctype=='goa' else print('CPR data')\n",
    "ds_bird.close()\n",
    "#ds_bird['lon'] = (ds)bird['lon'] + 180) % 360 - 180  #make -180 to 180\n",
    "ds_bird_save = ds_bird.copy(deep=True)\n",
    "ds_bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bird',ds_bird.lon.min().data,ds_bird.lon.max().data)\n",
    "for name in data:\n",
    "    ds = data[name]\n",
    "    print(name,ds.lon.min().data,ds.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate all data with eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ds_data=data['aviso']\n",
    "#tem = ds_data.interp(time=ds_bird.time64,lat=ds_bird.lat,lon=ds_bird.lon,assume_sorted=False)\n",
    "#tem.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data,clim = get_data()\n",
    "ilen_bird1 = len(ds_bird.track)\n",
    "ilen_bird2 = len(ds_bird.observation_number)\n",
    "\n",
    "ds_bird = ds_bird_save.copy(deep=True)\n",
    "for name in data:\n",
    "    ds_data=data[name]\n",
    "    print('data',name)\n",
    "    if name=='color': \n",
    "        continue\n",
    "    for var in ds_data:\n",
    "        var_tem=var\n",
    "        ds_bird[var_tem]=xr.DataArray(np.nan*np.empty((ilen_bird1,ilen_bird2), \n",
    "                                                      dtype=str(ds_data[var].dtype)), \n",
    "                                      coords={'track': ds_bird.track,'observation_number':ds_bird.observation_number},\n",
    "                                      dims=('track','observation_number'))\n",
    "        ds_bird[var_tem].attrs=ds_data[var].attrs\n",
    "    if name=='topo':\n",
    "        dtvar = np.timedelta64(9,'h') if name=='mur_sst' else np.timedelta64(0,'h')   # if MUR SST need to change to 9am\n",
    "        tem = ds_data.interp(lat=ds_bird.lat,lon=ds_bird.lon,assume_sorted=False)\n",
    "        tem = tem.load()\n",
    "        for var in ds_data:\n",
    "            var_tem=var\n",
    "            ds_bird[var_tem][:,:]=tem[var].data\n",
    "    else:\n",
    "        dtvar = np.timedelta64(9,'h') if name=='mur_sst' else np.timedelta64(0,'h')   # if MUR SST need to change to 9am\n",
    "        tem = ds_data.interp(time=ds_bird.time64+dtvar,lat=ds_bird.lat,lon=ds_bird.lon,assume_sorted=False)\n",
    "        tem = tem.load()\n",
    "        for var in ds_data:\n",
    "            var_tem=var\n",
    "            ds_bird[var_tem][:,:]=tem[var].data\n",
    "    #output data\n",
    "    ds_bird.to_netcdf(filename_origin_out+name+'.nc')\n",
    "    print('output',filename_origin_out+name+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#color\n",
    "ilen_bird1 = len(ds_bird.track)\n",
    "ilen_bird2 = len(ds_bird.observation_number)\n",
    "\n",
    "ds_bird = ds_bird_save.copy(deep=True)\n",
    "for name in data:\n",
    "    ds_data=data[name]\n",
    "    print('data',name)\n",
    "    if (name=='aviso') or (name=='wnd') or (name=='sst') or (name=='topo'): \n",
    "        continue\n",
    "    for var in ds_data:\n",
    "        var_tem=var\n",
    "        ds_bird[var_tem]=xr.DataArray(np.nan*np.empty((ilen_bird1,ilen_bird2), \n",
    "                                                      dtype=str(ds_data[var].dtype)), \n",
    "                                      coords={'track': ds_bird.track,'observation_number':ds_bird.observation_number},\n",
    "                                      dims=('track','observation_number'))\n",
    "        ds_bird[var_tem].attrs=ds_data[var].attrs\n",
    "    if name=='topo':\n",
    "        dtvar = np.timedelta64(9,'h') if name=='sst' else np.timedelta64(0,'h')   # if MUR SST need to change to 9am\n",
    "        tem = ds_data.interp(lat=ds_bird.lat,lon=ds_bird.lon,assume_sorted=False)\n",
    "        tem = tem.load()\n",
    "        for var in ds_data:\n",
    "            var_tem=var\n",
    "            ds_bird[var_tem][:,:]=tem[var].data\n",
    "    else:\n",
    "        for i in range(ilen_bird1):\n",
    "            dtvar = np.timedelta64(9,'h') if name=='sst' else np.timedelta64(0,'h')   # if MUR SST need to change to 9am\n",
    "            try:\n",
    "                tem = ds_data.interp(time=ds_bird.time64[i,:]+dtvar,lat=ds_bird.lat[i,:],lon=ds_bird.lon[i,:],assume_sorted=False)\n",
    "                tem = tem.load()\n",
    "            except:\n",
    "                print('exception',name)\n",
    "                pass\n",
    "            for var in ds_data:\n",
    "                var_tem=var\n",
    "                ds_bird[var_tem][i,:]=tem[var].data\n",
    "            if int(i/50)*50==i:\n",
    "                #output data\n",
    "                ds_bird.to_netcdf(filename_origin_out+name+'.nc')\n",
    "                print('output',i,filename_origin_out+name+'.nc')\n",
    "    #output data\n",
    "    ds_bird.to_netcdf(filename_origin_out+name+'.nc')\n",
    "    print('output',filename_origin_out+name+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=417-419,494,497,498,690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(690,ilen_bird1):\n",
    "    dtvar = np.timedelta64(9,'h') if name=='sst' else np.timedelta64(0,'h')   # if MUR SST need to change to 9am\n",
    "    try:\n",
    "        tem = ds_data.interp(time=ds_bird.time64[i,:]+dtvar,lat=ds_bird.lat[i,:],lon=ds_bird.lon[i,:],assume_sorted=False)\n",
    "        tem = tem.load()\n",
    "    except:\n",
    "        print('exception at ',i)\n",
    "        pass # doing nothing on exception\n",
    "    for var in ds_data:\n",
    "        var_tem=var\n",
    "        ds_bird[var_tem][i,:]=tem[var].data\n",
    "    if int(i/50)*50==i:\n",
    "        #output data\n",
    "        ds_bird.to_netcdf(filename_origin_out+name+'.nc')\n",
    "        print('output',i,filename_origin_out+name+'.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# put all the files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "#put it all together and create a csv file\n",
    "#print(filename_origin_out)\n",
    "#filename = glob.glob('f:/data/project_data/NASA_biophysical/collocated_data/'+filename_origin_out+'*.nc')\n",
    "filename = glob.glob(filename_origin_out+'*.nc')\n",
    "#filename = filename[1:]\n",
    "filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename[0])\n",
    "ds = xr.open_dataset(filename[0])\n",
    "for iname in range(1,len(filename)):\n",
    "    print(filename[iname])\n",
    "    ds2 = xr.open_dataset(filename[iname])\n",
    "    for var in ds2:\n",
    "        if not var in ds:\n",
    "            print(var)\n",
    "            ds[var]=ds2[var]               \n",
    "ds.to_netcdf(filename_origin_out+'all'+'.nc')\n",
    "df_bird = ds.to_dataframe()\n",
    "df_bird.to_csv(filename_origin_out+'all'+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if int(i/10)*10==i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ds.lon,ds.lat,c=ds.sla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_out=adir_data + 'All CPR Sample catalogue with eddy info_version2020_10_07.nc'\n",
    "ds_eddy.to_netcdf(filename_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bird.to_netcdf(filename_origin_out+name+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW, the point of this is to look up collocated eddy information and get the history of the data.  Steps are:\n",
    "1. Read in list of collocated eddies.\n",
    "2. Create list of unique eddy ID\n",
    "3. Read in full eddy database and select eddy id\n",
    "4. collocate environmental data for entire eddy history\n",
    "5. save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_all = get_all_eddy(filename_aviso_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eddy = get_eddy(filename_cpr_all)\n",
    "ds_eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all data where eddy radius < distance to eddy\n",
    "#find unique id & create a list\n",
    "subset = ds_eddy.where(ds_eddy.cpr_eddy_data_radius-ds_eddy.cpr_eddy_data_distance>0,drop=True)\n",
    "_, index = np.unique(subset['cpr_eddy_data_track'], return_index=True)\n",
    "eddy_list = subset['cpr_eddy_data_track'][index]\n",
    "print(eddy_list[0])\n",
    "#(subset.cpr_eddy_data_radius-subset.cpr_eddy_data_distance).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,clim = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddy_list.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddy_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in eddy_list:\n",
    "    if id in [118997,133297,149182,150094,181103,223608,233920,237425,241096]:\n",
    "        print(id.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#for ieddy in eddy_list:\n",
    "for ieddy in range(eddy_list.size):\n",
    "    if ieddy<8:\n",
    "        continue\n",
    "    subset = ds_all.where(ds_all.track==eddy_list[ieddy],drop=True)\n",
    "    tt=np.empty(subset.obs.size,dtype='datetime64[ns]') \n",
    "    for i in range(subset.obs.size):\n",
    "        tstr=str(subset.time[i].dt.year.data)+'-'+str(subset.time[i].dt.month.data).zfill(2)+'-'+str(subset.time[i].dt.day.data).zfill(2)\n",
    "        tem=np.datetime64(tstr)\n",
    "        tt[i]=tem\n",
    "    subset['time']=xr.DataArray(tt,dims=['obs'])  \n",
    "    for name in data:\n",
    "        ds_data=data[name]\n",
    "        if name=='topo':\n",
    "            continue\n",
    "        print('name',name)\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']\n",
    "            subset[var_tem]=subset.latitude.copy(deep=True)*np.NaN\n",
    "            subset[var_tem].attrs=ds_data[var].attrs\n",
    "        print('var',var_tem)\n",
    "        for i in range(subset.latitude.size):\n",
    "            lat1,lat2=subset.latitude[i].data-1,subset.latitude[i].data+1\n",
    "            lon1,lon2=subset.longitude[i].data-1,subset.longitude[i].data+1\n",
    "            #interp in time and select region around lat/lon to subset before loading data\n",
    "            #interp doesn't work on chunked dims so rechunk\n",
    "            ds_data2 = ds_data.chunk({'time':ds_data.time.size,'lat':ds_data[var].chunks[1],'lon':ds_data[var].chunks[2]})\n",
    "            #ds_data2.interp(time=subset.time[i].data)\n",
    "            tem = ds_data2.interp(time=subset.time[i].data).sel(lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "            tem = tem.interp(lat=subset.latitude[i].data,lon=subset.longitude[i].data)\n",
    "            for var in ds_data:\n",
    "                var_tem=ds_data[var].attrs['var_name']\n",
    "                subset[var_tem][i]=tem[var]\n",
    "    for name in clim:\n",
    "        ds_data=clim[name]\n",
    "        print('name',name)\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "            subset[var_tem]=subset.latitude.copy(deep=True)*np.NaN\n",
    "            subset[var_tem].attrs=ds_data[var].attrs\n",
    "        print('var',var_tem)\n",
    "        for i in range(subset.latitude.size):\n",
    "            lat1,lat2=subset.latitude[i].data-1,subset.latitude[i].data+1\n",
    "            lon1,lon2=subset.longitude[i].data-1,subset.longitude[i].data+1\n",
    "            #interp in time and select region around lat/lon to subset before loading data\n",
    "            ds_data2 = ds_data.chunk({'dayofyear':ds_data.time.size,'lat':ds_data[var].chunks[1],'lon':ds_data[var].chunks[2]})\n",
    "            #ds_data2.interp(time=subset.time[i].data)\n",
    "            tem = ds_data2.sel(dayofyear=subset.time[i].dt.dayofyear.data).sel(lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "            tem = tem.interp(lat=subset.latitude[i].data,lon=subset.longitude[i].data)\n",
    "            for var in ds_data:\n",
    "                var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "                subset[var_tem][i]=tem[var]\n",
    "    ds_topo=data['topo']\n",
    "    subset['ETOPO_depth']=ds_topo.z.interp(lat=subset.latitude,lon=subset.longitude,method='nearest')   \n",
    "    filename_out='F:/data/NASA_biophysical//collocated_data/eddy_collocated_data'+str(ieddy).zfill(8)+'.nc'\n",
    "    subset.to_netcdf(filename_out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ds_data2 = ds_data.chunk({'time':ds_data.time.size,'lat':ds_data[var].chunks[1],'lon':ds_data[var].chunks[2]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data[var].chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eddy(filename):\n",
    "    ds_eddy = xr.open_dataset(filename)\n",
    "    tt=np.empty(ds_eddy.etopo_depth.size,dtype='datetime64[ns]') \n",
    "    for i in range(ds_eddy.etopo_depth.size):\n",
    "        tstr=str(ds_eddy.cpr_sample_year[i].data)+'-'+str(ds_eddy.cpr_sample_month[i].data).zfill(2)+'-'+str(ds_eddy.cpr_sample_day[i].data).zfill(2)\n",
    "        tem=np.datetime64(tstr)\n",
    "        tt[i]=tem\n",
    "    ds_eddy['cpr_sample_time']=xr.DataArray(tt,dims=['z'])\n",
    "    return ds_eddy\n",
    "\n",
    "def get_all_eddy(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    ds['longitude'] = (ds['longitude'] + 180) % 360 - 180\n",
    "    ds_eddy = ds\n",
    "#    tt=np.empty(ds_eddy.obs.size,dtype='datetime64[ns]') \n",
    "#    for i in range(ds_eddy.obs.size):\n",
    "#        tstr=str(ds_eddy.time[i].dt.year.data)+'-'+str(ds_eddy.time[i].dt.month.data).zfill(2)+'-'+str(ds_eddy.time[i].dt.day.data).zfill(2)\n",
    "#        tem=np.datetime64(tstr)\n",
    "#        tt[i]=tem\n",
    "#    ds_eddy['cpr_sample_time']=xr.DataArray(tt,dims=['obs'])\n",
    "    return ds_eddy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(nrows=1, figsize=(6, 5.4))\n",
    "im = ax1.imshow(ds_topo.z[7000:9500,0:4500], interpolation='bilinear',vmin=-7000.0, vmax=1.0,aspect='auto',origin='lower')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
