{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "####################you will need to change some paths here!#####################\n",
    "#list of input files\n",
    "filename_cpr='f:/data/NASA_biophysical/CPR_data/All CPR Sample catalogue.xlsx'\n",
    "filename_northpac_eddies='F:/data/NASA_biophysical/aviso/eddy_trajectory_19930101_20170106_north_pacific.nc'\n",
    "filename_cpr_eddy='F:/data/NASA_biophysical/collocated_data/eddy_cpr_data_north_pacific.nc'\n",
    "filename_eddy='F:/data/NASA_biophysical/collocated_data/eddy_ranking_data_north_pacific.nc'\n",
    "#output files\n",
    "filename_cpr_expanded='f:/data/NASA_biophysical/collocated_data/All CPR Sample catalogue with eddy info2.xlsx'\n",
    "filename_cpr_expanded_netcdf='f:/data/NASA_biophysical/collocated_data/All CPR Sample catalogue with eddy info2.nc'\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to read in data and put in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to get all the data at once, use same years for climatology for all data\n",
    "def get_data():\n",
    "    \n",
    "    #climatology years\n",
    "    cyr1,cyr2='1993-01-01','2018-12-31'\n",
    "    \n",
    "    # CCMP test\n",
    "    dir_pattern_zarr = 'F:/data/sat_data/ccmp/zarr/'\n",
    "    ds= xr.open_zarr(dir_pattern_zarr)\n",
    "    ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "    ds.coords['lon'] = (ds.coords['lon'] + 180) % 360 - 180\n",
    "    ds_ccmp = ds.sortby(ds.lon)\n",
    "    ds_ccmp = ds_ccmp.drop('nobs')\n",
    "    for var in ds_ccmp:\n",
    "        tem = ds_ccmp[var].attrs\n",
    "        tem['var_name']='ccmp_'+str(var)\n",
    "        ds_ccmp[var].attrs=tem\n",
    "    ds_ccmp_clim = ds_ccmp.sel(time=slice(cyr1,cyr2))\n",
    "    ds_ccmp_clim = ds_ccmp_clim.groupby('time.dayofyear').mean('time',keep_attrs=True,skipna=False)\n",
    "    \n",
    "    # AVISO test\n",
    "    dir_pattern_zarr = 'F:/data/sat_data/aviso/zarr/'\n",
    "    ds= xr.open_zarr(dir_pattern_zarr)\n",
    "    ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "    ds.coords['lon'] = (ds.coords['lon'] + 180) % 360 - 180\n",
    "    ds_aviso = ds.sortby(ds.lon).drop({'lat_bnds','lon_bnds','crs','err'})\n",
    "    for var in ds_aviso:\n",
    "        tem = ds_aviso[var].attrs\n",
    "        tem['var_name']='aviso_'+str(var)\n",
    "        ds_aviso[var].attrs=tem\n",
    "    ds_aviso_clim = ds_aviso.sel(time=slice(cyr1,cyr2))\n",
    "    ds_aviso_clim = ds_aviso_clim.groupby('time.dayofyear').mean('time',keep_attrs=True,skipna=False)    \n",
    "\n",
    "    #sst\n",
    "    dir_pattern_zarr = 'F:/data/sst/cmc/zarr/'\n",
    "    ds_sst= xr.open_zarr(dir_pattern_zarr)\n",
    "    ds_sst = ds_sst.drop({'analysis_error','mask','sea_ice_fraction'})\n",
    "    tem = ds_sst.analysed_sst.attrs\n",
    "    tem['var_name']='cmc_sst'\n",
    "    ds_sst.analysed_sst.attrs=tem\n",
    "    ds_sst_clim = ds_sst.sel(time=slice(cyr1,cyr2))\n",
    "    ds_sst_clim = ds_sst_clim.groupby('time.dayofyear').mean('time',keep_attrs=True,skipna=False)\n",
    "    \n",
    "    #get bathymetry from ETOPO1\n",
    "    fname_topo = 'F:/data/topo/ETOPO1_Ice_g_gmt4.grd'\n",
    "    ds = xr.open_dataset(fname_topo)\n",
    "#    x = ds.x  #21601\n",
    "#    y = ds.y   #10801\n",
    "#    topo = ds.z  #(10801, 21601)\n",
    "    ds_topo = ds.rename_dims({'x':'lon','y':'lat'}).rename({'x':'lon','y':'lat'})\n",
    "    tem = ds_topo.z.attrs\n",
    "    tem['var_name']='etopo_depth'\n",
    "    ds_topo.z.attrs=tem\n",
    "#    ds_topo\n",
    "\n",
    "    #put data into a dictionary\n",
    "    data_dict={'aviso':ds_aviso,\n",
    "               'wnd':ds_ccmp,\n",
    "               'sst':ds_sst,\n",
    "              'topo':ds_topo}\n",
    "    clim_dict={'aviso_clim':ds_aviso_clim,\n",
    "               'wnd_clim':ds_ccmp_clim,\n",
    "               'sst_clim':ds_sst_clim}\n",
    "  \n",
    "    return data_dict,clim_dict\n",
    "\n",
    "def get_eddy():\n",
    "    filename='F:/data/NASA_biophysical//collocated_data/All CPR Sample catalogue with eddy info4.nc'\n",
    "    ds_eddy = xr.open_dataset(filename)\n",
    "    tt=np.empty(ds_eddy.z.size,dtype='datetime64[ns]') \n",
    "    for i in range(ds_eddy.z.size):\n",
    "        tstr=str(ds_eddy.cpr_sample_year[i].data)+'-'+str(ds_eddy.cpr_sample_month[i].data).zfill(2)+'-'+str(ds_eddy.cpr_sample_day[i].data).zfill(2)\n",
    "        tem=np.datetime64(tstr)\n",
    "        tt[i]=tem\n",
    "    ds_eddy['cpr_sample_time']=xr.DataArray(tt,dims=['z'])\n",
    "    return ds_eddy\n",
    "\n",
    "def get_all_eddy():\n",
    "    filename_aviso='f:/data/NASA_biophysical/aviso/eddy_trajectory_19930101_20170106.nc'   #From AVISO  website\n",
    "    ds = xr.open_dataset(filename_aviso)\n",
    "    ds['longitude'] = (ds['longitude'] + 180) % 360 - 180\n",
    "    ds_eddy = ds\n",
    "#    tt=np.empty(ds_eddy.obs.size,dtype='datetime64[ns]') \n",
    "#    for i in range(ds_eddy.obs.size):\n",
    "#        tstr=str(ds_eddy.time[i].dt.year.data)+'-'+str(ds_eddy.time[i].dt.month.data).zfill(2)+'-'+str(ds_eddy.time[i].dt.day.data).zfill(2)\n",
    "#        tem=np.datetime64(tstr)\n",
    "#        tt[i]=tem\n",
    "#    ds_eddy['cpr_sample_time']=xr.DataArray(tt,dims=['obs'])\n",
    "    return ds_eddy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,clim = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eddy = get_eddy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename_aviso='f:/data/NASA_biophysical/aviso/eddy_trajectory_19930101_20170106.nc'   #From AVISO  website\n",
    "#ds_eddy = xr.open_dataset(filename_aviso)\n",
    "#ds_eddy\n",
    "#ds_all = get_all_eddy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate all data with eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in data:\n",
    "    ds_data=data[name]\n",
    "    if name=='topo':\n",
    "        continue\n",
    "    print('name',name)\n",
    "    for var in ds_data:\n",
    "        var_tem=ds_data[var].attrs['var_name']\n",
    "        ds_eddy[var_tem]=ds_eddy.cpr_sample_ccmp_uwnd.copy(deep=True)*np.NaN\n",
    "        ds_eddy[var_tem].attrs=ds_data[var].attrs\n",
    "    print('var',var_tem)\n",
    "    for i in range(2): #ds_eddy.z.size):\n",
    "        lat1,lat2=ds_eddy.cpr_sample_lat[i].data-1,ds_eddy.cpr_sample_lat[i].data+1\n",
    "        lon1,lon2=ds_eddy.cpr_sample_lon[i].data-1,ds_eddy.cpr_sample_lon[i].data+1\n",
    "        #interp in time and select region around lat/lon to subset before loading data\n",
    "        tem = ds_data.interp(time=ds_eddy.cpr_sample_time[i].data).sel(lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "        tem = tem.interp(lat=ds_eddy.cpr_sample_lat[i].data,lon=ds_eddy.cpr_sample_lon[i].data)\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']\n",
    "            ds_eddy[var_tem]=tem[var]\n",
    "for name in clim:\n",
    "    ds_data=clim[name]\n",
    "    print('name',name)\n",
    "    for var in ds_data:\n",
    "        var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "        ds_eddy[var_tem]=ds_eddy.cpr_sample_ccmp_uwnd.copy(deep=True)*np.NaN\n",
    "        ds_eddy[var_tem].attrs=ds_data[var].attrs\n",
    "    print('var',var_tem)\n",
    "    for i in range(2): #ds_eddy.z.size):\n",
    "        lat1,lat2=ds_eddy.cpr_sample_lat[i].data-1,ds_eddy.cpr_sample_lat[i].data+1\n",
    "        lon1,lon2=ds_eddy.cpr_sample_lon[i].data-1,ds_eddy.cpr_sample_lon[i].data+1\n",
    "        #interp in time and select region around lat/lon to subset before loading data\n",
    "        tem = ds_data.sel(dayofyear=ds_eddy.cpr_sample_time[i].dt.dayofyear.data).sel(lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "        tem = tem.interp(lat=ds_eddy.cpr_sample_lat[i].data,lon=ds_eddy.cpr_sample_lon[i].data)\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "            ds_eddy[var_tem]=tem[var]\n",
    "\n",
    "ds_topo=data['topo']\n",
    "ds_eddy['ETOPO_depth']=ds_topo.z.interp(lat=ds_eddy.cpr_sample_lat,lon=ds_eddy.cpr_sample_lon,method='nearest')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_out='F:/data/NASA_biophysical//collocated_data/All CPR Sample catalogue with eddy info_version2020_04_21.nc'\n",
    "ds_eddy.to_netcdf(filename_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW, the point of this is to look up collocated eddy information and get the history of the data.  Steps are:\n",
    "1. Read in list of collocated eddies.\n",
    "2. Create list of unique eddy ID\n",
    "3. Read in full eddy database and select eddy id\n",
    "4. collocate environmental data for entire eddy history\n",
    "5. save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all = get_all_eddy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eddy = get_eddy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'cpr_eddy_data_track_id' ()>\n",
      "array(81964.)\n"
     ]
    }
   ],
   "source": [
    "#drop all data where eddy radius < distance to eddy\n",
    "#find unique id & create a list\n",
    "subset = ds_eddy.where(ds_eddy.cpr_eddy_data_radius-ds_eddy.cpr_eddy_data_distance>0,drop=True)\n",
    "_, index = np.unique(subset['cpr_eddy_data_track_id'], return_index=True)\n",
    "eddy_list = subset['cpr_eddy_data_track_id'][index]\n",
    "print(eddy_list[0])\n",
    "#(subset.cpr_eddy_data_radius-subset.cpr_eddy_data_distance).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,clim = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eddy_list.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name aviso\n",
      "var aviso_vgosa\n",
      "name wnd\n",
      "var ccmp_vwnd\n",
      "name sst\n",
      "var cmc_sst\n",
      "name aviso_clim\n",
      "var aviso_vgosa_clim\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-f727544a1c94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mlon1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlon2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m#interp in time and select region around lat/lon to subset before loading data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mds_data2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lon'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mds_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[1;31m#ds_data2.interp(time=subset.time[i].data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mtem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_data2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdayofyear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdayofyear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlat1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlat2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlon1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlon2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\satenv\\lib\\site-packages\\xarray\\core\\common.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    231\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         raise AttributeError(\n\u001b[1;32m--> 233\u001b[1;33m             \u001b[1;34m\"{!r} object has no attribute {!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         )\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'time'"
     ]
    }
   ],
   "source": [
    "#for ieddy in eddy_list:\n",
    "for ieddy in range(eddy_list.size):\n",
    "    if ieddy<8:\n",
    "        continue\n",
    "    subset = ds_all.where(ds_all.track==eddy_list[ieddy],drop=True)\n",
    "    tt=np.empty(subset.obs.size,dtype='datetime64[ns]') \n",
    "    for i in range(subset.obs.size):\n",
    "        tstr=str(subset.time[i].dt.year.data)+'-'+str(subset.time[i].dt.month.data).zfill(2)+'-'+str(subset.time[i].dt.day.data).zfill(2)\n",
    "        tem=np.datetime64(tstr)\n",
    "        tt[i]=tem\n",
    "    subset['time']=xr.DataArray(tt,dims=['obs'])  \n",
    "    for name in data:\n",
    "        ds_data=data[name]\n",
    "        if name=='topo':\n",
    "            continue\n",
    "        print('name',name)\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']\n",
    "            subset[var_tem]=subset.latitude.copy(deep=True)*np.NaN\n",
    "            subset[var_tem].attrs=ds_data[var].attrs\n",
    "        print('var',var_tem)\n",
    "        for i in range(subset.latitude.size):\n",
    "            lat1,lat2=subset.latitude[i].data-1,subset.latitude[i].data+1\n",
    "            lon1,lon2=subset.longitude[i].data-1,subset.longitude[i].data+1\n",
    "            #interp in time and select region around lat/lon to subset before loading data\n",
    "            #interp doesn't work on chunked dims so rechunk\n",
    "            ds_data2 = ds_data.chunk({'time':ds_data.time.size,'lat':ds_data[var].chunks[1],'lon':ds_data[var].chunks[2]})\n",
    "            #ds_data2.interp(time=subset.time[i].data)\n",
    "            tem = ds_data2.interp(time=subset.time[i].data).sel(lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "            tem = tem.interp(lat=subset.latitude[i].data,lon=subset.longitude[i].data)\n",
    "            for var in ds_data:\n",
    "                var_tem=ds_data[var].attrs['var_name']\n",
    "                subset[var_tem][i]=tem[var]\n",
    "    for name in clim:\n",
    "        ds_data=clim[name]\n",
    "        print('name',name)\n",
    "        for var in ds_data:\n",
    "            var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "            subset[var_tem]=subset.latitude.copy(deep=True)*np.NaN\n",
    "            subset[var_tem].attrs=ds_data[var].attrs\n",
    "        print('var',var_tem)\n",
    "        for i in range(subset.latitude.size):\n",
    "            lat1,lat2=subset.latitude[i].data-1,subset.latitude[i].data+1\n",
    "            lon1,lon2=subset.longitude[i].data-1,subset.longitude[i].data+1\n",
    "            #interp in time and select region around lat/lon to subset before loading data\n",
    "            ds_data2 = ds_data.chunk({'time':ds_data.time.size,'lat':ds_data[var].chunks[1],'lon':ds_data[var].chunks[2]})\n",
    "            #ds_data2.interp(time=subset.time[i].data)\n",
    "            tem = ds_data2.sel(dayofyear=subset.time[i].dt.dayofyear.data).sel(lat=slice(lat1,lat2),lon=slice(lon1,lon2)).load()\n",
    "            tem = tem.interp(lat=subset.latitude[i].data,lon=subset.longitude[i].data)\n",
    "            for var in ds_data:\n",
    "                var_tem=ds_data[var].attrs['var_name']+'_clim'\n",
    "                subset[var_tem][i]=tem[var]\n",
    "    ds_topo=data['topo']\n",
    "    subset['ETOPO_depth']=ds_topo.z.interp(lat=subset.latitude,lon=subset.longitude,method='nearest')   \n",
    "    filename_out='F:/data/NASA_biophysical//collocated_data/eddy_collocated_data'+str(ieddy).zfill(8)+'.nc'\n",
    "    subset.to_netcdf(filename_out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ds_data2 = ds_data.chunk({'time':ds_data.time.size,'lat':ds_data[var].chunks[1],'lon':ds_data[var].chunks[2]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 508),\n",
       " (180, 180, 180, 180),\n",
       " (180, 180, 180, 180, 180, 180, 180, 180))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_data[var].chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray &#x27;vgosa&#x27; (time: 9508, lat: 720, lon: 1440)&gt;\n",
       "dask.array&lt;getitem, shape=(9508, 720, 1440), dtype=float64, chunksize=(1000, 180, 180), chunktype=numpy.ndarray&gt;\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * lon      (lon) float32 -179.875 -179.625 -179.375 ... 179.625 179.875\n",
       "  * time     (time) datetime64[ns] 1993-01-01 1993-01-02 ... 2019-01-12\n",
       "Attributes:\n",
       "    comment:        The geostrophic velocity anomalies are referenced to the ...\n",
       "    grid_mapping:   crs\n",
       "    long_name:      Geostrophic velocity anomalies: meridian component\n",
       "    standard_name:  surface_geostrophic_northward_sea_water_velocity_assuming...\n",
       "    units:          m/s\n",
       "    var_name:       aviso_vgosa</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray 'vgosa' (time: 9508, lat: 720, lon: 1440)>\n",
       "dask.array<getitem, shape=(9508, 720, 1440), dtype=float64, chunksize=(1000, 180, 180), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * lon      (lon) float32 -179.875 -179.625 -179.375 ... 179.625 179.875\n",
       "  * time     (time) datetime64[ns] 1993-01-01 1993-01-02 ... 2019-01-12\n",
       "Attributes:\n",
       "    comment:        The geostrophic velocity anomalies are referenced to the ...\n",
       "    grid_mapping:   crs\n",
       "    long_name:      Geostrophic velocity anomalies: meridian component\n",
       "    standard_name:  surface_geostrophic_northward_sea_water_velocity_assuming...\n",
       "    units:          m/s\n",
       "    var_name:       aviso_vgosa"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_data[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(nrows=1, figsize=(6, 5.4))\n",
    "im = ax1.imshow(ds_topo.z[7000:9500,0:4500], interpolation='bilinear',vmin=-7000.0, vmax=1.0,aspect='auto',origin='lower')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
